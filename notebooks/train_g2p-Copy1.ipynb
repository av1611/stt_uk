{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import time\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "# tf.config.experimental.set_visible_devices(devices=gpus[0], device_type='GPU')\n",
    "# tf.config.experimental.set_memory_growth(device=gpus[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/phone_data.json.gz'\n",
    "RANDOM_SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'  # tf 2.0+\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gzip2json(path):\n",
    "    with gzip.GzipFile(path, 'r') as fin:\n",
    "        data = json.loads(fin.read().decode('utf-8'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gzip2json(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* chr(769) we are replacing stress near letters\n",
    "* adding a for cases word transcription missed it, for such word like *-тися -> *-тись etc.\n",
    "\n",
    "TODO: think about removing dash '-' as it has no phoneme at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160385/160385 [00:01<00:00, 137515.00it/s]\n"
     ]
    }
   ],
   "source": [
    "words, letters, phones = [], [], []\n",
    "ignored = 0\n",
    "\n",
    "for item in tqdm(data):\n",
    "    word = item['word'].lower()\n",
    "    ltrs = [letter for letter in word]\n",
    "    phns = [letter.replace(chr(769), '') for letter in item['phones']]\n",
    "    \n",
    "    if set(['#', \"{и^е}'\", \"{о^у}'\", \"о'\", \"ґ'\"]).intersection(phns):\n",
    "        ignored += 1\n",
    "        continue\n",
    "    \n",
    "    words.append(word)\n",
    "    letters.append(ltrs)\n",
    "    phones.append(phns)\n",
    "\n",
    "data_preprocessed = list(zip(words, letters, phones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignored: 80\n"
     ]
    }
   ],
   "source": [
    "print('ignored:', ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:  політтехнологічний\n",
      "letters:  ['п', 'о', 'л', 'і', 'т', 'т', 'е', 'х', 'н', 'о', 'л', 'о', 'г', 'і', 'ч', 'н', 'и', 'й']\n",
      "phonemes:  ['п', 'о', \"л'\", 'і', 'т:', '{е^и}', 'х', 'н', 'о', 'л', '{о^у}', \"г'\", 'і', 'ч', 'н', 'и', 'й']\n"
     ]
    }
   ],
   "source": [
    "sample = random.choice(data_preprocessed)\n",
    "print(f'word: ', sample[0])\n",
    "print(f'letters: ', sample[1])\n",
    "print(f'phonemes: ', sample[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(array):\n",
    "    for item in array:\n",
    "        if isinstance(item, list):\n",
    "            yield from flatten(item)\n",
    "        else:\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceTokenizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.inverse_vocab = {}\n",
    "        \n",
    "    def fit(self, sequence):\n",
    "        self.index2word = dict(enumerate(sorted(set(flatten(sequence)))))\n",
    "        self.word2index = {v:k for k,v in self.index2word.items()}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        res = []\n",
    "        for line in X:\n",
    "            res.append([self.word2index[item] for item in line])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = letters\n",
    "target_sequences = phones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = [['<start>'] + item + ['<end>'] for item in input_sequences]\n",
    "target_sequences = [['<start>'] + item + ['<end>'] for item in target_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_tokenizer = SequenceTokenizer().fit(input_sequences)\n",
    "phone_tokenizer = SequenceTokenizer().fit(target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove '<start>' and '<end>' for lstm training\n",
    "# input_sequences = [item[1:-1] for item in input_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_letters = sorted(set(flatten(input_sequences)))\n",
    "unique_phones = sorted(set(flatten(target_sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens = len(unique_letters)\n",
    "num_decoder_tokens = len(unique_phones)\n",
    "max_encoder_seq_length = max([len(item) for item in input_sequences])\n",
    "max_decoder_seq_length = max([len(item) for item in target_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 160305\n",
      "Number of unique input tokens: 37\n",
      "Number of unique output tokens: 94\n",
      "Max sequence length for inputs: 35\n",
      "Max sequence length for outputs: 33\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_sequences))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = letter_tokenizer.transform(input_sequences)\n",
    "y = phone_tokenizer.transform(target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160305, 35) (160305, 33)\n"
     ]
    }
   ],
   "source": [
    "X_padded = tf.keras.preprocessing.sequence.pad_sequences(X, padding='post')\n",
    "y_padded = tf.keras.preprocessing.sequence.pad_sequences(y, padding='post')\n",
    "print(X_padded.shape, y_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160305, 35, 37) (160305, 33, 94)\n"
     ]
    }
   ],
   "source": [
    "X_ohe = np.stack([tf.keras.utils.to_categorical(item, num_classes=num_encoder_tokens) for item in X_padded])\n",
    "y_ohe = np.stack([tf.keras.utils.to_categorical(item, num_classes=num_decoder_tokens) for item in y_padded])\n",
    "print(X_ohe.shape, y_ohe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((144274, 35), (16031, 35))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_padded_train, X_padded_test, y_padded_train, y_padded_test = train_test_split(X_padded, y_padded, \n",
    "                                                                                test_size=0.1, \n",
    "                                                                                random_state=RANDOM_SEED)\n",
    "X_padded_train.shape, X_padded_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((144274, 35, 37), (16031, 35, 37))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ohe_train, X_ohe_test, y_ohe_train, y_ohe_test = train_test_split(X_ohe, y_ohe, \n",
    "                                                                    test_size=0.1, \n",
    "                                                                    random_state=RANDOM_SEED)\n",
    "X_ohe_train.shape, X_ohe_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras tutorial preprocessing\n",
    "\n",
    "# encoder_input_data = np.zeros(\n",
    "#     (len(input_sequences), max_encoder_seq_length, num_encoder_tokens),\n",
    "#     dtype='float32')\n",
    "# decoder_input_data = np.zeros(\n",
    "#     (len(input_sequences), max_decoder_seq_length, num_decoder_tokens),\n",
    "#     dtype='float32')\n",
    "# decoder_target_data = np.zeros(\n",
    "#     (len(input_sequences), max_decoder_seq_length, num_decoder_tokens),\n",
    "#     dtype='float32')\n",
    "\n",
    "\n",
    "# for i, (input_sequence, target_sequence) in enumerate(zip(input_sequences, target_sequences)):\n",
    "#     _ = []\n",
    "#     for t, char in enumerate(input_sequence):\n",
    "#         encoder_input_data[i, t, letter_tokenizer.word2index[char]] = 1.\n",
    "#         _.append(letter_tokenizer.word2index[char])\n",
    "#     for t, char in enumerate(target_sequence):\n",
    "#         decoder_input_data[i, t, phone_tokenizer.word2index[char]] = 1.\n",
    "#         if t > 0:\n",
    "#             decoder_target_data[i, t - 1, phone_tokenizer.word2index[char]] = 1.\n",
    "            \n",
    "# print(encoder_input_data.shape, decoder_input_data.shape, decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seq2seq LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "latent_dim = 128\n",
    "\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(None, num_encoder_tokens))\n",
    "encoder = tf.keras.layers.LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = tf.keras.layers.Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs, name='seq2seq_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"seq2seq_lstm\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 37)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 94)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 128), (None, 84992       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 128),  114176      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 94)     12126       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 211,294\n",
      "Trainable params: 211,294\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.2, verbose=True),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=7, min_delta=1e-3, verbose=True), \n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='seq2seq_lstm.h5', save_best_only=True, verbose=True)\n",
    "]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144274 samples, validate on 16031 samples\n",
      "144192/144274 [============================>.] - ETA: 0s - loss: 0.1563 - accuracy: 0.9716\n",
      "Epoch 00001: val_loss improved from inf to 0.00168, saving model to seq2seq_lstm.h5\n",
      "144274/144274 [==============================] - 93s 645us/sample - loss: 0.1562 - accuracy: 0.9716 - val_loss: 0.0017 - val_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_ohe_train, y_ohe_train], y_ohe_train,\n",
    "    batch_size=64,\n",
    "    epochs=1,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=([X_ohe_test, y_ohe_test], y_ohe_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_lstm = tf.keras.models.load_model('seq2seq_lstm.h5')\n",
    "\n",
    "encoder_inputs = seq2seq_lstm.input[0]   # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = seq2seq_lstm.layers[2].output   # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = seq2seq_lstm.input[1]   # input_2\n",
    "decoder_state_input_h = tf.keras.layers.Input(shape=(latent_dim,), name='input_3')\n",
    "decoder_state_input_c = tf.keras.layers.Input(shape=(latent_dim,), name='input_4')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = seq2seq_lstm.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = seq2seq_lstm.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = tf.keras.models.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(x, tokenizer, max_len, num_classes=None):\n",
    "    if isinstance(x, str):\n",
    "        x = [x]\n",
    "    x = tokenizer.transform(x)\n",
    "    x = tf.keras.preprocessing.sequence.pad_sequences(x, maxlen=max_len)\n",
    "    if num_classes:\n",
    "        x = np.stack([tf.keras.utils.to_categorical(item, num_classes=num_classes) for item in x])\n",
    "    return x\n",
    "    \n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, phone_tokenizer.word2index['<start>']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = phone_tokenizer.index2word[sampled_token_index]\n",
    "        decoded_sentence += [sampled_char]\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: аудиторія\n",
      "sample transcription: ['<start>', '<start>', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', 'а', '<end>', '<end>', '<end>', '<end>']\n"
     ]
    }
   ],
   "source": [
    "sample = 'аудиторія'\n",
    "\n",
    "decoded_sample = decode_sequence(vectorize_sentence(sample, \n",
    "                                                    tokenizer=letter_tokenizer, \n",
    "                                                    max_len=max_encoder_seq_length, \n",
    "                                                    num_classes=num_encoder_tokens))\n",
    "\n",
    "print('sample:', sample)\n",
    "print('sample transcription:', decoded_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seq2seq GRU with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(X_padded_train)\n",
    "BUFFER_SIZE_VAL = len(X_padded_test)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "steps_per_epoch = BUFFER_SIZE // BATCH_SIZE\n",
    "steps_per_epoch_val = BUFFER_SIZE_VAL // BATCH_SIZE\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_size_encoder = len(letter_tokenizer.word2index) + 1\n",
    "vocab_size_decoder = len(phone_tokenizer.word2index) + 1\n",
    "max_length_input, max_length_output = X_padded_train.shape[1], y_padded_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_padded_train, y_padded_train)).shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_padded_test, y_padded_test)).shuffle(BUFFER_SIZE)\n",
    "test_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([128, 35]), TensorShape([128, 33]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_output_batch = next(iter(train_dataset))\n",
    "example_input_batch.shape, example_output_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create tf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (128, 35, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (128, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size_encoder, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (128, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (128, 35, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (128, 95)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_size_decoder, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train tf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, out, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([phone_tokenizer.word2index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the output as the next input\n",
    "        for t in range(1, out.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(out[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(out[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(out.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def val_step(inp, out, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([phone_tokenizer.word2index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the output as the next input\n",
    "        for t in range(1, out.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(out[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(out[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(out.shape[1]))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-920f7cca15a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2360\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                           converted_func)\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m                     \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m                     \u001b[0moptional_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautograph_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m                     \u001b[0muser_requested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m                 ))\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/tmp4k05r4ky.py\u001b[0m in \u001b[0;36mtf__train_step\u001b[0;34m(inp, out, enc_hidden)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mdec_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m           \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'dec_hidden'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dec_input'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py\u001b[0m in \u001b[0;36mfor_stmt\u001b[0;34m(iter_, extra_test, body, get_state, set_state, init_vars, basic_symbol_names, composite_symbol_names)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_py_for_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/operators/control_flow.py\u001b[0m in \u001b[0;36m_py_for_stmt\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mextra_test\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextra_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/tmp4k05r4ky.py\u001b[0m in \u001b[0;36mloop_body\u001b[0;34m(iterates, dec_hidden, dec_input, loss)\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0;32mdef\u001b[0m \u001b[0mloop_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mdec_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_cached_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misbuiltin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0m_attach_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m           optional_features=optional_features)\n\u001b[1;32m    233\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/tmpsc4a023v.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, x, hidden, enc_output)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontext_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_cached_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misbuiltin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0m_attach_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m       last_output, outputs, runtime, states = self._defun_gru_call(\n\u001b[0;32m--> 422\u001b[0;31m           inputs, initial_state, training, mask, row_lengths)\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36m_defun_gru_call\u001b[0;34m(self, inputs, initial_state, training, mask, sequence_lengths)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m       last_output, outputs, new_h, runtime = gru_with_backend_selection(\n\u001b[0;32m--> 483\u001b[0;31m           **normal_gru_kwargs)\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_h\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mgru_with_backend_selection\u001b[0;34m(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, activation, recurrent_activation, sequence_lengths)\u001b[0m\n\u001b[1;32m    762\u001b[0m   \u001b[0;31m# grappler will kick in during session execution to optimize the graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m   \u001b[0mlast_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefun_standard_gru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m   \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_cudnn_gru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlast_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2729\u001b[0m   \u001b[0mconcrete_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2730\u001b[0m   \u001b[0mconcrete_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2731\u001b[0;31m   \u001b[0mconcrete_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_gradient_functions_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2732\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36madd_gradient_functions_to_graph\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m     forward_function, backward_function = (\n\u001b[0;32m-> 1821\u001b[0;31m         self._delayed_rewrite_functions.forward_backward())\n\u001b[0m\u001b[1;32m   1822\u001b[0m     \u001b[0mforward_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m     \u001b[0mbackward_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_backward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mforward_backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m     \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_forward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_function_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_construct_forward_backward\u001b[0;34m(self, num_doutputs)\u001b[0m\n\u001b[1;32m    662\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m           \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m           func_graph=backwards_graph)\n\u001b[0m\u001b[1;32m    665\u001b[0m       \u001b[0mbackwards_graph_captures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackwards_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternal_captures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m       captures_from_forward = [\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                           converted_func)\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_backprop_function\u001b[0;34m(*grad_ys)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0mgrad_ys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m             src_graph=self._func_graph)\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    710\u001b[0m       \u001b[0;31m# Update pending count for the inputs of op and enqueue ready ops.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m       _UpdatePendingAndEnqueueReady(grads, op, queue, pending_count, loop_state,\n\u001b[0;32m--> 712\u001b[0;31m                                     xs_set)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mloop_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_UpdatePendingAndEnqueueReady\u001b[0;34m(grads, op, queue, pending_count, loop_state, xs_set)\u001b[0m\n\u001b[1;32m    738\u001b[0m       \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpending_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontrol_flow_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsLoopSwitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mcontrol_flow_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsLoopExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;31m# if x is an exit without real gradient, defer processing them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0mgrad_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetGradState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbefore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_util.py\u001b[0m in \u001b[0;36mIsLoopExit\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mIsLoopExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;34m\"\"\"Return true if `op` is an Exit.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Exit\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"RefExit\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2218\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m     \u001b[0;34m\"\"\"The type of the op (e.g. `\"MatMul\"`).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2220\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationOpType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "history = {}\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    start_time = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{n_epochs}\")\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(test_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = val_step(inp, targ, enc_hidden)\n",
    "        total_val_loss += batch_loss\n",
    "    \n",
    "    print('loss: {loss:.4f}\\tval_loss: {val_loss:.4f}\\texec_time: {exec_time:.2f} seconds\\n'.format(\n",
    "            loss = total_loss / steps_per_epoch, \n",
    "            val_loss = total_val_loss / steps_per_epoch_val,\n",
    "            exec_time = time.time() - start_time))\n",
    "    \n",
    "    history['train'] = history.get('train', []) + [total_loss.numpy() / steps_per_epoch]\n",
    "    history['val'] = history.get('val', []) + [total_val_loss.numpy() / steps_per_epoch_val]\n",
    "            \n",
    "#     saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2seq LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = letters\n",
    "target_sequences = phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sequences = [['<start>'] + item + ['<end>'] for item in target_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_letters = sorted(set(flatten(input_sequences)))\n",
    "# unique_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_phones = sorted(set(flatten(target_sequences)))\n",
    "# unique_phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens = len(unique_letters)\n",
    "num_decoder_tokens = len(unique_phones)\n",
    "max_encoder_seq_length = max([len(item) for item in input_sequences])\n",
    "max_decoder_seq_length = max([len(item) for item in target_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of samples:', len(input_sequences))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(unique_letters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(unique_phones)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_sequences), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_sequences), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_sequences), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1, var2, var3, var4 = train_test_split(list(zip(encoder_input_data, decoder_input_data)), decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_sequence, target_sequence) in enumerate(zip(input_sequences, target_sequences)):\n",
    "    for t, char in enumerate(input_sequence):\n",
    "        encoder_input_data[i, t, letter_tokenizer.inverse_vocab[char]] = 1.\n",
    "    for t, char in enumerate(target_sequence):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=(None, num_encoder_tokens))\n",
    "encoder = tf.keras.layers.LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = tf.keras.layers.Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs, name='seq2seq_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.2, verbose=True),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=7, min_delta=1e-3, verbose=True), \n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='seq2seq_lstm.h5', save_best_only=True, verbose=True)\n",
    "]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    [encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('seq2seq_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = model.input[0]   # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output   # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]   # input_2\n",
    "decoder_state_input_h = tf.keras.layers.Input(shape=(latent_dim,), name='input_3')\n",
    "decoder_state_input_c = tf.keras.layers.Input(shape=(latent_dim,), name='input_4')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = tf.keras.models.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['<start>']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += [sampled_char]\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '<end>' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_index = 123\n",
    "\n",
    "input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('-')\n",
    "print('Input sentence:', input_sequences[seq_index])\n",
    "print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(X_padded_train)\n",
    "BUFFER_SIZE_VAL = len(X_padded_test)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "steps_per_epoch = BUFFER_SIZE // BATCH_SIZE\n",
    "steps_per_epoch_val = BUFFER_SIZE_VAL // BATCH_SIZE\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_size_encoder = len(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "\n",
    "# from keras.models import Model\n",
    "# from keras.layers import Input, LSTM, Dense\n",
    "# import numpy as np\n",
    "\n",
    "# batch_size = 64  # Batch size for training.\n",
    "# epochs = 100  # Number of epochs to train for.\n",
    "# latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "# num_samples = 10000  # Number of samples to train on.\n",
    "# # Path to the data txt file on disk.\n",
    "# data_path = 'fra-eng/fra.txt'\n",
    "\n",
    "# # Vectorize the data.\n",
    "# input_texts = []\n",
    "# target_texts = []\n",
    "# input_characters = set()\n",
    "# target_characters = set()\n",
    "# with open(data_path, 'r', encoding='utf-8') as f:\n",
    "#     lines = f.read().split('\\n')\n",
    "# for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "#     input_text, target_text = line.split('\\t')\n",
    "#     # We use \"tab\" as the \"start sequence\" character\n",
    "#     # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "#     target_text = '\\t' + target_text + '\\n'\n",
    "#     input_texts.append(input_text)\n",
    "#     target_texts.append(target_text)\n",
    "#     for char in input_text:\n",
    "#         if char not in input_characters:\n",
    "#             input_characters.add(char)\n",
    "#     for char in target_text:\n",
    "#         if char not in target_characters:\n",
    "#             target_characters.add(char)\n",
    "\n",
    "# input_characters = sorted(list(input_characters))\n",
    "# target_characters = sorted(list(target_characters))\n",
    "# num_encoder_tokens = len(input_characters)\n",
    "# num_decoder_tokens = len(target_characters)\n",
    "# max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "# max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "# print('Number of samples:', len(input_texts))\n",
    "# print('Number of unique input tokens:', num_encoder_tokens)\n",
    "# print('Number of unique output tokens:', num_decoder_tokens)\n",
    "# print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "# print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "# input_token_index = dict(\n",
    "#     [(char, i) for i, char in enumerate(input_characters)])\n",
    "# target_token_index = dict(\n",
    "#     [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "# encoder_input_data = np.zeros(\n",
    "#     (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "#     dtype='float32')\n",
    "# decoder_input_data = np.zeros(\n",
    "#     (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "#     dtype='float32')\n",
    "# decoder_target_data = np.zeros(\n",
    "#     (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "#     dtype='float32')\n",
    "\n",
    "# for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "#     for t, char in enumerate(input_text):\n",
    "#         encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "#     encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "#     for t, char in enumerate(target_text):\n",
    "#         # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "#         decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "#         if t > 0:\n",
    "#             # decoder_target_data will be ahead by one timestep\n",
    "#             # and will not include the start character.\n",
    "#             decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "#     decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "#     decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "# Define an input sequence and process it.\n",
    "# encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "# encoder = LSTM(latent_dim, return_state=True)\n",
    "# encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# # We discard `encoder_outputs` and only keep the states.\n",
    "# encoder_states = [state_h, state_c]\n",
    "\n",
    "# # Set up the decoder, using `encoder_states` as initial state.\n",
    "# decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# # We set up our decoder to return full output sequences,\n",
    "# # and to return internal states as well. We don't use the\n",
    "# # return states in the training model, but we will use them in inference.\n",
    "# decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "# decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "#                                      initial_state=encoder_states)\n",
    "# decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# # Define the model that will turn\n",
    "# # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=(None, max_len_encoder))\n",
    "\n",
    "encoder = tf.keras.layers.LSTM(128, return_state=True)\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = tf.keras.layers.Input(shape=(None, max_len_decoder))\n",
    "\n",
    "decoder_lstm = tf.keras.layers.LSTM(128, return_sequences=True, return_state=True)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = tf.keras.layers.Dense(len(unique_phones), activation='softmax')\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.2, verbose=True),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=5, min_delta=1e-3, verbose=True), \n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='g2p_lstm.h5', save_best_only=True, verbose=True)\n",
    "]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(x=[X, y], y=y,\n",
    "# #                     class_weight='balanced',\n",
    "#                     epochs=2,\n",
    "#                     batch_size=32,\n",
    "#                     callbacks=callbacks,\n",
    "# #                     validation_data=([X_dev, y_dev], y_dev),\n",
    "#                     verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2seq with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, test_input, train_output, test_output = X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = train_input.shape[0]\n",
    "BUFFER_SIZE_VAL = test_input.shape[0]\n",
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = BUFFER_SIZE // BATCH_SIZE\n",
    "steps_per_epoch_val = BUFFER_SIZE_VAL // BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_size = len(tokenizer.vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_input, max_length_output = train_input.shape[1], train_output.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_input, train_output)).shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_input, test_output)).shuffle(BUFFER_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_batch, example_output_batch = next(iter(train_dataset))\n",
    "example_input_batch.shape, example_output_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, out, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([tokenizer.vocab['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the output as the next input\n",
    "        for t in range(1, out.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(out[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(out[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(out.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def val_step(inp, out, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([tokenizer.vocab['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the output as the next input\n",
    "        for t in range(1, out.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(out[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(out[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(out.shape[1]))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "history = {}\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    start_time = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{n_epochs}\")\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(test_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = val_step(inp, targ, enc_hidden)\n",
    "        total_val_loss += batch_loss\n",
    "    \n",
    "    print('loss: {loss:.4f}\\tval_loss: {val_loss:.4f}\\texec_time: {exec_time:.2f} seconds\\n'.format(\n",
    "            loss = total_loss / steps_per_epoch, \n",
    "            val_loss = total_val_loss / steps_per_epoch_val,\n",
    "            exec_time = time.time() - start_time))\n",
    "    \n",
    "    history['train'] = history.get('train', []) + [total_loss.numpy() / steps_per_epoch]\n",
    "    history['val'] = history.get('val', []) + [total_val_loss.numpy() / steps_per_epoch_val]\n",
    "            \n",
    "#     saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['train'])\n",
    "plt.plot(history['val'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('val')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(word):\n",
    "    \n",
    "    ltrs = [letter for letter in word]\n",
    "    if ltrs[0] != '<start>':\n",
    "        ltrs = ['<start>'] + ltrs\n",
    "    if ltrs[-1] != '<end>':\n",
    "        ltrs = ltrs + ['<end>']\n",
    "    \n",
    "    attention_plot = np.zeros((max_length_output, max_length_input))\n",
    "\n",
    "    inputs = tokenizer.transform([ltrs])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=max_length_input,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([tokenizer.vocab['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_output):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += tokenizer.inverse_vocab[predicted_id] + ' '\n",
    "\n",
    "        if tokenizer.inverse_vocab[predicted_id] == '<end>':\n",
    "            return result, \" \".join(ltrs), attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "    return result, \" \".join(ltrs), attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def paraphrase(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted paraphrase: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate('аахен')[0].replace('<end>', '').split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrase('автівка')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "\n",
    "for item in tqdm(data_preprocessed[:1000]):\n",
    "    prediction = evaluate(item[0])[0].replace('<end>', '').strip()\n",
    "    true = \" \".join(item[2][1:-1])\n",
    "    y_pred.append((true, prediction, int(prediction==true)))\n",
    "#     y_pred.append(evaluate(item[0])[0].replace('<end>', '').strip())\n",
    "#     y_test.append(\" \".join(item[2][1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pred = [item[:2] for item in y_pred if item[2] == 1]\n",
    "false_pred = [item[:2] for item in y_pred if item[2] == 0]\n",
    "accuracy = len(true_pred) / len(y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:p36] *",
   "language": "python",
   "name": "conda-env-p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
